#ftp相关配置
ftp {
  ftp-host = "localhost"
  ftp-port = 21
  ftp-user = "cookeem"
  ftp-pass = "man8080"
  #远程ftp根目录
  ftp-root = "/Volumes/Share/Download/HSS"
  #本地ftp目录
  ftp-localroot = "download"
}

#redis相关配置
redis {
  hosts = [
    {
      redis-host = "localhost"
      redis-port = 30001
    },
    {
      redis-host = "localhost"
      redis-port = 30002
    },
    {
      redis-host = "localhost"
      redis-port = 30003
    },
    {
      redis-host = "localhost"
      redis-port = 30004
    },
    {
      redis-host = "localhost"
      redis-port = 30005
    },
    {
      redis-host = "localhost"
      redis-port = 30006
    }
  ]
}

#kafka相关配置
kafka {
  zookeeper-uri = "localhost:2181"
  brokers-list = "localhost:9092,localhost:9093,localhost:9094"
  kafka-files-topic = "hss-files-topic"
  consume-group = "hss-consume-group"
  kafka-records-topic = "hss-records-topic"
  kafka-num-partitions = 1
  kafka-replication = 1
}

#elasticsearch相关配置
elasticsearch {
  cluster-name = "hss_es"
  hosts = [
    {
      host = "localhost"
      port = 9300
    }
  ]
  index-name = "hss"
  type-name = "hss_record"
  number-of-shards = 2
  number-of-replicas = 1
}

#akka http configuration
akka.http {
  server {
    remote-address-header = on
    raw-request-uri-header = on
  }
}

#http端口
http {
  port = 9870
}

#akka相关配置,请勿修改
akka {
  loglevel = "ERROR"
  log-dead-letters = off
  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
    kryo  { #Kryo序列化的配置
      type = "graph"
      idstrategy = "incremental"
      serializer-pool-size = 16
      buffer-size = 4096
      use-manifests = false
      implicit-registration-logging = true
      kryo-trace = false
      classes = [
        "java.lang.String",
        "scala.Some",
        "scala.None$",
        "cmgd.zenghj.hss.actor.DirectiveStopMember$",
        "cmgd.zenghj.hss.actor.DirectiveListDir$",
        "cmgd.zenghj.hss.actor.DirectiveListFile",
        "cmgd.zenghj.hss.actor.DirectiveStat$",
        "cmgd.zenghj.hss.actor.DirectiveStatResult",
        "cmgd.zenghj.hss.actor.DirectiveFtpKafka",
        "cmgd.zenghj.hss.actor.DirectiveFtpKafkaResult"
      ]
    }
    serializers { #配置可能使用的序列化算法
      java = "akka.serialization.JavaSerializer"
      kryo = "com.romix.akka.serialization.kryo.KryoSerializer"
    }
    serialization-bindings { #配置序列化类与算法的绑定
      "java.lang.String"=kryo
      "scala.Some"=kryo
      "scala.None$"=kryo
      "cmgd.zenghj.hss.actor.DirectiveStopMember$"=kryo
      "cmgd.zenghj.hss.actor.DirectiveListDir$"=kryo
      "cmgd.zenghj.hss.actor.DirectiveListFile"=kryo
      "cmgd.zenghj.hss.actor.DirectiveStat$"=kryo
      "cmgd.zenghj.hss.actor.DirectiveStatResult"=kryo
      "cmgd.zenghj.hss.actor.DirectiveFtpKafka"=kryo
      "cmgd.zenghj.hss.actor.DirectiveFtpKafkaResult"=kryo
    }
  }
  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "localhost"
      port = 0
    }
  }
  cluster {
    #akka cluster seed nodes配置,请根据实际情况修改
    seed-nodes = [
      "akka.tcp://hss-cluster@localhost:2551",
      "akka.tcp://hss-cluster@localhost:2552"]
    auto-down-unreachable-after = 5s
  }
}

#akka cluster路由器配置
cluster-router {
  master {
    #master 计划任务刷新时间间隔
    schedule-interval = 20
    #统计各个GetFileWorker的时间间隔
    stat-interval = 10
  }
  #目录文件列表路由器设置
  listfile-router {
    pool-size = 5
    lower-bound = 5
    upper-bound = 20
    max-instances-pernode = 3
  }
  getfile-worker {
    #一个getfile-worker启动多少个stream
    stream-count = 2
  }
}

